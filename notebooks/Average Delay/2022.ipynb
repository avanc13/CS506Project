{"cells":[{"cell_type":"code","execution_count":1,"id":"2721d2d3-1e3e-42f8-bd18-bd98c90245d1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":908,"referenced_widgets":["b1c1924480b54eea806d13fed3e7fb39","19f5d0d34b854466bf7b739d4a9df387","26f81a7882984d0f9f357fdb691fa255"]},"id":"2721d2d3-1e3e-42f8-bd18-bd98c90245d1","executionInfo":{"status":"error","timestamp":1761493381385,"user_tz":240,"elapsed":59159,"user":{"displayName":"Sofia Chaoui","userId":"02126645620424037100"}},"outputId":"d88f4d69-1504-4873-b992-97b9fb11318e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading 2022 ZIP…\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=17T8ExVFkYQ-9IJsCfQ-ZSl2y5n71w_FW\n","From (redirected): https://drive.google.com/uc?id=17T8ExVFkYQ-9IJsCfQ-ZSl2y5n71w_FW&confirm=t&uuid=5cabfc93-fa5b-4e13-87bd-bf64b0fdd80c\n","To: /tmp/mbta_2022.zip\n","100%|██████████| 366M/366M [00:03<00:00, 115MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting to /tmp/mbta_2022 …\n","Found 12 CSV files for 2022.\n"]},{"output_type":"display_data","data":{"text/plain":["FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1c1924480b54eea806d13fed3e7fb39"}},"metadata":{}},{"output_type":"error","ename":"ConversionException","evalue":"Conversion Error: CSV Error on Line: 1990194\nOriginal Line: \nNA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\nError when converting column \"service_date\". Could not convert string \"NA\" to 'DATE'\n\nColumn service_date is being converted as type DATE\nThis type was auto-detected from the CSV file.\nPossible solutions:\n* Override the type for this column manually by setting the type explicitly, e.g., types={'service_date': 'VARCHAR'}\n* Set the sample size to a larger value to enable the auto-detection to scan more values, e.g., sample_size=-1\n* Use a COPY statement to automatically derive types from an existing table.\n* Check whether the null string value is set correctly (e.g., nullstr = 'N/A')\n\n  file = /tmp/mbta_2022/MBTA_Bus_Arrival_Departure_Times_2022/MBTA-Bus-Arrival-Departure-Times_2022-08.csv\n  delimiter = , (Auto-Detected)\n  quote = \" (Auto-Detected)\n  escape = \" (Auto-Detected)\n  new_line = \\r\\n (Auto-Detected)\n  header = true (Set By User)\n  skip_rows = 0 (Auto-Detected)\n  comment = (empty) (Auto-Detected)\n  strict_mode = true (Auto-Detected)\n  date_format =  (Auto-Detected)\n  timestamp_format =  (Auto-Detected)\n  null_padding = 0\n  sample_size = 20480\n  ignore_errors = false\n  all_varchar = 0\n\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mConversionException\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4292418539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# ---- DuckDB load ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m con.execute(\"\"\"\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0mCREATE\u001b[0m \u001b[0mOR\u001b[0m \u001b[0mREPLACE\u001b[0m \u001b[0mTABLE\u001b[0m \u001b[0mevents\u001b[0m \u001b[0mAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mSELECT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0mread_csv_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munion_by_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mConversionException\u001b[0m: Conversion Error: CSV Error on Line: 1990194\nOriginal Line: \nNA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\nError when converting column \"service_date\". Could not convert string \"NA\" to 'DATE'\n\nColumn service_date is being converted as type DATE\nThis type was auto-detected from the CSV file.\nPossible solutions:\n* Override the type for this column manually by setting the type explicitly, e.g., types={'service_date': 'VARCHAR'}\n* Set the sample size to a larger value to enable the auto-detection to scan more values, e.g., sample_size=-1\n* Use a COPY statement to automatically derive types from an existing table.\n* Check whether the null string value is set correctly (e.g., nullstr = 'N/A')\n\n  file = /tmp/mbta_2022/MBTA_Bus_Arrival_Departure_Times_2022/MBTA-Bus-Arrival-Departure-Times_2022-08.csv\n  delimiter = , (Auto-Detected)\n  quote = \" (Auto-Detected)\n  escape = \" (Auto-Detected)\n  new_line = \\r\\n (Auto-Detected)\n  header = true (Set By User)\n  skip_rows = 0 (Auto-Detected)\n  comment = (empty) (Auto-Detected)\n  strict_mode = true (Auto-Detected)\n  date_format =  (Auto-Detected)\n  timestamp_format =  (Auto-Detected)\n  null_padding = 0\n  sample_size = 20480\n  ignore_errors = false\n  all_varchar = 0\n\n"]}],"source":["# =========================\n","# MBTA Bus Reliability — 2022 (DuckDB, adaptive schema)\n","# =========================\n","!pip install -q duckdb gdown pyzipper\n","\n","import os, glob, zipfile, shutil\n","import duckdb as ddb\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# ---- Config ----\n","YEAR = \"2022\"\n","FILE_ID = \"17T8ExVFkYQ-9IJsCfQ-ZSl2y5n71w_FW\"  # Google Drive file id for 2018 ZIP\n","EXCLUDED_ROUTES = ('8007','7001','RAD','191','192','193','194','743','746_')\n","TARGET_ROUTES   = (\"22\",\"29\",\"15\",\"45\",\"28\",\"44\",\"42\",\"17\",\"23\",\"31\",\"26\",\"111\",\"24\",\"33\",\"14\")\n","ON_TIME_THRESHOLD_MIN = 0.5\n","\n","TMP_DIR = f\"/tmp/mbta_{YEAR}\"\n","ZIP_PATH = f\"/tmp/mbta_{YEAR}.zip\"\n","DB_PATH  = f\"/tmp/mbta_{YEAR}.duckdb\"\n","\n","# Clean temp & create dirs\n","if os.path.exists(TMP_DIR):\n","    shutil.rmtree(TMP_DIR)\n","os.makedirs(TMP_DIR, exist_ok=True)\n","\n","# ---- Download & extract ZIP ----\n","import gdown\n","url = f\"https://drive.google.com/uc?id={FILE_ID}\"\n","print(f\"Downloading {YEAR} ZIP…\")\n","gdown.download(url, ZIP_PATH, quiet=False)\n","\n","def extract_zip_with_fallback(zip_path, out_dir):\n","    try:\n","        with zipfile.ZipFile(zip_path, \"r\") as zf:\n","            zf.extractall(out_dir)\n","            return\n","    except (zipfile.BadZipFile, NotImplementedError):\n","        import pyzipper\n","        with pyzipper.AESZipFile(zip_path) as zf:\n","            zf.extractall(out_dir)\n","\n","print(f\"Extracting to {TMP_DIR} …\")\n","extract_zip_with_fallback(ZIP_PATH, TMP_DIR)\n","\n","csvs = glob.glob(os.path.join(TMP_DIR, \"**\", \"*.csv\"), recursive=True)\n","if not csvs:\n","    raise RuntimeError(f\"No CSV files found in {TMP_DIR}\")\n","print(f\"Found {len(csvs)} CSV files for {YEAR}.\")\n","\n","# ---- DuckDB load ----\n","con = ddb.connect(DB_PATH)\n","\n","# detect the actual columns in the CSV files\n","sample_df = con.execute(\"\"\"\n","  SELECT * FROM read_csv_auto(?, union_by_name=true, header=true, filename=true, sample_size=-1) LIMIT 1\n","\"\"\", [csvs]).df()\n","\n","print(\"Detected columns:\", sample_df.columns.tolist())\n","\n","# Load with explicit type overrides for problematic columns\n","con.execute(\"\"\"\n","  CREATE OR REPLACE TABLE events AS\n","  SELECT * FROM read_csv_auto(?,\n","    union_by_name=true,\n","    header=true,\n","    filename=true,\n","    sample_size=-1,\n","    all_varchar=true  -- Read all columns as VARCHAR first, then we'll cast later\n","  );\n","\"\"\", [csvs])\n","\n","# ---- Inspect available columns ----\n","cols = con.execute(\"PRAGMA table_info('events')\").df()[\"name\"].str.lower().tolist()\n","def pick(*candidates):\n","    for c in candidates:\n","        if c and c.lower() in cols:\n","            return c\n","    return None\n","\n","# Flexible column mapping\n","route_col      = pick(\"route_id\",\"route\")\n","stop_col       = pick(\"stop_id\",\"time_point_id\",\"stop\")\n","trip_col       = pick(\"trip_id\",\"half_trip_id\")\n","direction_col  = pick(\"direction_id\",\"direction\")\n","service_date_c = pick(\"service_date\",\"date\")\n","scheduled_col  = pick(\"scheduled\",\"scheduled_time\",\"sched_time\")\n","actual_col     = pick(\"actual\",\"actual_time\",\"time\",\"timestamp\",\"departure_time\",\"arrival_time\")\n","delay_col_raw  = pick(\"delay_min\",\"lateness\",\"delay\",\"earliness\")  # 'earliness' will be negated\n","\n","# Build SQL expressions safely\n","route_expr     = f\"CAST({route_col} AS VARCHAR)\" if route_col else \"NULL\"\n","stop_expr      = f\"CAST({stop_col} AS VARCHAR)\" if stop_col else \"NULL\"\n","trip_expr      = f\"CAST({trip_col} AS VARCHAR)\" if trip_col else \"NULL\"\n","direction_expr = f\"CAST({direction_col} AS VARCHAR)\" if direction_col else \"NULL\"\n","service_expr   = f\"CAST({service_date_c} AS VARCHAR)\" if service_date_c else \"NULL\"\n","sched_expr     = f\"CAST({scheduled_col} AS VARCHAR)\" if scheduled_col else \"NULL\"\n","actual_expr    = f\"CAST({actual_col} AS VARCHAR)\" if actual_col else \"NULL\"\n","\n","# Delay expression:\n","#  - if 'delay_min' or 'lateness'/'delay' exists, use it;\n","#  - if only 'earliness' exists, delay_min = -earliness (early => negative delay);\n","\n","def to_double(colname):\n","    # Strip thousands separators; try-cast to DOUBLE\n","    return f\"TRY_CAST(REPLACE({colname}, ',', '') AS DOUBLE)\"\n","\n","delay_expr = \"NULL\"\n","if delay_col_raw:\n","    if delay_col_raw.lower() == \"earliness\":\n","        delay_expr = f\"(-1.0 * {to_double(delay_col_raw)})\"\n","    else:\n","        delay_expr = to_double(delay_col_raw)\n","\n","# Some datasets already provide headway; detect it\n","headway_col = pick(\"headway\",\"headway_min\",\"avg_headway\",\"wait_time\")\n","\n","# ---------- Build 'clean' view ----------\n","# Parse timestamps only if we have the needed columns; otherwise leave NULL and rely on provided headway\n","# --- Safe timestamp parsing that trims extra timestamps and offsets ---\n","# Extract only the first datetime part (up to first timezone or extra timestamp)\n","# --- Always define time_parse safely as a static SQL block ---\n","# --- Robust timestamp parsing: supports datetime, time-less dates, MM/DD/YYYY too ---\n","# Robust timestamp parsing:\n","# - pull just the first date/time via REGEXP_EXTRACT\n","# - NULLIF('', '') -> NULL\n","# - TRY_STRPTIME(...) returns NULL instead of raising on bad/empty strings\n","time_parse = \"\"\"\n","    /* scheduled_dt */\n","    COALESCE(\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}[ T]\\\\d{2}:\\\\d{2}:\\\\d{2}'), ''), '%Y-%m-%d %H:%M:%S'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}[ T]\\\\d{2}:\\\\d{2}'),    ''), '%Y-%m-%d %H:%M'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}'),                     ''), '%Y-%m-%d'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '[0-9/]{10} \\\\d{2}:\\\\d{2}:\\\\d{2}'),          ''), '%m/%d/%Y %H:%M:%S'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '[0-9/]{10} \\\\d{2}:\\\\d{2}'),                 ''), '%m/%d/%Y %H:%M'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '[0-9/]{10}'),                               ''), '%m/%d/%Y')\n","    ) AS scheduled_dt,\n","\n","    /* actual_dt */\n","    COALESCE(\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}[ T]\\\\d{2}:\\\\d{2}:\\\\d{2}'), ''), '%Y-%m-%d %H:%M:%S'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}[ T]\\\\d{2}:\\\\d{2}'),    ''), '%Y-%m-%d %H:%M'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}'),                     ''), '%Y-%m-%d'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '[0-9/]{10} \\\\d{2}:\\\\d{2}:\\\\d{2}'),          ''), '%m/%d/%Y %H:%M:%S'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '[0-9/]{10} \\\\d{2}:\\\\d{2}'),                 ''), '%m/%d/%Y %H:%M'),\n","      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '[0-9/]{10}'),                               ''), '%m/%d/%Y')\n","    ) AS actual_dt\n","\"\"\"\n","\n","\n","\n","clean_sql = f\"\"\"\n","CREATE OR REPLACE VIEW clean AS\n","WITH base AS (\n","  SELECT\n","    {route_expr}     AS route_id,\n","    {stop_expr}      AS stop_id,\n","    {trip_expr}      AS trip_raw,\n","    {direction_expr} AS direction_id,\n","    {service_expr}   AS service_date,\n","    {sched_expr}     AS scheduled_raw,\n","    {actual_expr}    AS actual_raw,\n","    {delay_expr}     AS delay_from_file,\n","    {\"TRY_CAST(\" + headway_col + \" AS DOUBLE)\" if headway_col else \"NULL\"} AS headway_from_file\n","  FROM events\n","),\n","ts AS (\n","  SELECT\n","    *,\n","    {time_parse}\n","  FROM base\n","),\n","enriched AS (\n","  SELECT\n","    route_id, stop_id, trip_raw, direction_id, service_date,\n","    scheduled_dt, actual_dt, headway_from_file,\n","    /* Prefer provided delay; else compute from timestamps if possible */\n","    COALESCE(\n","      delay_from_file,\n","      CASE WHEN scheduled_dt IS NOT NULL AND actual_dt IS NOT NULL\n","           THEN DATE_DIFF('minute', scheduled_dt, actual_dt)::DOUBLE\n","           ELSE NULL END\n","    ) AS delay_min\n","  FROM ts\n","),\n","sequenced AS (\n","  SELECT\n","    *,\n","    CASE\n","      WHEN trip_raw IS NOT NULL THEN trip_raw\n","      ELSE (\n","        COALESCE(route_id,'') || '-' || COALESCE(service_date,'') || '-' || COALESCE(direction_id,'') || '-t' ||\n","        CAST(\n","          ROW_NUMBER() OVER (\n","            PARTITION BY route_id, service_date, direction_id\n","            ORDER BY COALESCE(actual_dt, scheduled_dt)\n","          ) AS BIGINT\n","        )\n","      )\n","    END AS trip_key\n","  FROM enriched\n",")\n","SELECT *\n","FROM sequenced\n","WHERE route_id NOT IN ({\",\".join(\"'\" + r + \"'\" for r in EXCLUDED_ROUTES)})\n","\"\"\"\n","con.execute(clean_sql)\n","\n","# ---------- Q1: End-to-end travel time by route ----------\n","# If we don't have timestamps, this will return empty; that's OK (some sources only have headway/delay).\n","e2e_df = con.execute(\"\"\"\n","  WITH trip_bounds AS (\n","    SELECT route_id, trip_key,\n","           MIN(actual_dt) AS t_min, MAX(actual_dt) AS t_max\n","    FROM clean\n","    WHERE actual_dt IS NOT NULL AND trip_key IS NOT NULL\n","    GROUP BY route_id, trip_key\n","  )\n","  SELECT route_id,\n","         AVG(DATE_DIFF('minute', t_min, t_max))::DOUBLE AS avg_e2e_min,\n","         COUNT(*) AS n_trips\n","  FROM trip_bounds\n","  GROUP BY route_id\n","  ORDER BY avg_e2e_min DESC\n","\"\"\").df()\n","\n","if not e2e_df.empty:\n","    plt.figure()\n","    plt.bar(e2e_df[\"route_id\"].head(25), e2e_df[\"avg_e2e_min\"].head(25))\n","    plt.xticks(rotation=90)\n","    plt.ylabel(\"Minutes\")\n","    plt.title(f\"Average End-to-End Travel Time by Route (Top 25) — {YEAR}\")\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"ℹ️ Skipping E2E plot — no usable timestamps to compute trip durations.\")\n","\n","# ---------- Q2: Average wait (headway) — on-time vs delayed ----------\n","if headway_col:\n","    # Use provided headway, then split by status using delay_min\n","    headway_df = con.execute(f\"\"\"\n","      WITH hw AS (\n","        SELECT\n","          CASE WHEN ABS(delay_min) <= {ON_TIME_THRESHOLD_MIN} THEN 'on_time' ELSE 'delayed' END AS status,\n","          headway_from_file AS headway_min\n","        FROM clean\n","        WHERE headway_from_file IS NOT NULL\n","      )\n","      SELECT status, AVG(headway_min) AS avg_headway_min\n","      FROM hw\n","      GROUP BY status\n","      ORDER BY status\n","    \"\"\").df()\n","else:\n","    # Compute headway from timestamps (if available)\n","    headway_df = con.execute(f\"\"\"\n","      WITH ordered AS (\n","        SELECT\n","          route_id, stop_id, actual_dt, delay_min,\n","          LAG(actual_dt) OVER (PARTITION BY route_id, stop_id ORDER BY actual_dt) AS prev_dt\n","        FROM clean\n","        WHERE actual_dt IS NOT NULL\n","      )\n","      SELECT\n","        CASE WHEN ABS(delay_min) <= {ON_TIME_THRESHOLD_MIN} THEN 'on_time' ELSE 'delayed' END AS status,\n","        AVG(DATE_DIFF('minute', prev_dt, actual_dt))::DOUBLE AS avg_headway_min\n","      FROM ordered\n","      WHERE prev_dt IS NOT NULL\n","      GROUP BY status\n","      ORDER BY status\n","    \"\"\").df()\n","\n","if not headway_df.empty:\n","    plt.figure()\n","    plt.bar(headway_df[\"status\"], headway_df[\"avg_headway_min\"])\n","    plt.ylabel(\"Average Headway (min)\")\n","    plt.title(f\"Citywide Average Wait (Headway) — On-Time vs Delayed — {YEAR}\")\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"ℹ️ Skipping headway plot — neither provided headway nor timestamps available.\")\n","\n","# ---------- Q3: Citywide average delay ----------\n","city_delay_df = con.execute(\"\"\"\n","  SELECT AVG(delay_min)::DOUBLE AS citywide_avg_delay_min\n","  FROM clean\n","  WHERE delay_min IS NOT NULL\n","\"\"\").df()\n","print(f\"Citywide average delay ({YEAR}): {city_delay_df['citywide_avg_delay_min'][0] if not city_delay_df.empty else 'NA'} min\")\n","\n","# ---------- Q4: Average delay for target routes ----------\n","targets_clause = \"(\" + \",\".join(f\"'{r}'\" for r in TARGET_ROUTES) + \")\"\n","targets_df = con.execute(f\"\"\"\n","  SELECT route_id, AVG(delay_min)::DOUBLE AS avg_delay_min\n","  FROM clean\n","  WHERE delay_min IS NOT NULL\n","    AND route_id IN {targets_clause}\n","  GROUP BY route_id\n","  ORDER BY avg_delay_min DESC\n","\"\"\").df()\n","\n","if not targets_df.empty:\n","    plt.figure()\n","    plt.bar(targets_df[\"route_id\"], targets_df[\"avg_delay_min\"])\n","    plt.xticks(rotation=90)\n","    plt.ylabel(\"Average Delay (min)\")\n","    plt.title(f\"Average Delay — Target Routes — {YEAR}\")\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"ℹ️ No target-route delay results — check that those routes exist in 2018 after exclusions.\")\n","\n","# ---------- Save outputs ----------\n","os.makedirs(\"results\", exist_ok=True)\n","e2e_df.to_csv(f\"results/e2e_{YEAR}.csv\", index=False)\n","headway_df.to_csv(f\"results/headway_on_time_vs_delayed_{YEAR}.csv\", index=False)\n","city_delay_df.to_csv(f\"results/citywide_avg_delay_{YEAR}.csv\", index=False)\n","targets_df.to_csv(f\"results/avg_delay_targets_{YEAR}.csv\", index=False)\n","print(\"✅ Done. CSVs saved in ./results\")\n"]},{"cell_type":"code","execution_count":null,"id":"c4d5cbe7-7571-481d-a106-cd04b4d4385a","metadata":{"id":"c4d5cbe7-7571-481d-a106-cd04b4d4385a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"b1c1924480b54eea806d13fed3e7fb39":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_19f5d0d34b854466bf7b739d4a9df387","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26f81a7882984d0f9f357fdb691fa255","value":13}},"19f5d0d34b854466bf7b739d4a9df387":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"auto"}},"26f81a7882984d0f9f357fdb691fa255":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":"black","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}