{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2721d2d3-1e3e-42f8-bd18-bd98c90245d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2022 ZIP…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=17T8ExVFkYQ-9IJsCfQ-ZSl2y5n71w_FW\n",
      "From (redirected): https://drive.google.com/uc?id=17T8ExVFkYQ-9IJsCfQ-ZSl2y5n71w_FW&confirm=t&uuid=0a05e38f-d2ce-41d7-89ce-fcbd68b61f3e\n",
      "To: /tmp/mbta_2022.zip\n",
      "100%|████████████████████████████████████████| 366M/366M [00:05<00:00, 61.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting to /tmp/mbta_2022 …\n",
      "Found 12 CSV files for 2022.\n"
     ]
    },
    {
     "ename": "ConversionException",
     "evalue": "Conversion Error: CSV Error on Line: 1990194\nOriginal Line: \nNA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\nError when converting column \"service_date\". Could not convert string \"NA\" to 'DATE'\n\nColumn service_date is being converted as type DATE\nThis type was auto-detected from the CSV file.\nPossible solutions:\n* Override the type for this column manually by setting the type explicitly, e.g., types={'service_date': 'VARCHAR'}\n* Set the sample size to a larger value to enable the auto-detection to scan more values, e.g., sample_size=-1\n* Use a COPY statement to automatically derive types from an existing table.\n* Check whether the null string value is set correctly (e.g., nullstr = 'N/A')\n\n  file = /tmp/mbta_2022/MBTA_Bus_Arrival_Departure_Times_2022/MBTA-Bus-Arrival-Departure-Times_2022-08.csv\n  delimiter = , (Auto-Detected)\n  quote = \" (Auto-Detected)\n  escape = \" (Auto-Detected)\n  new_line = \\r\\n (Auto-Detected)\n  header = true (Set By User)\n  skip_rows = 0 (Auto-Detected)\n  comment = (empty) (Auto-Detected)\n  strict_mode = true (Auto-Detected)\n  date_format =  (Auto-Detected)\n  timestamp_format =  (Auto-Detected)\n  null_padding = 0\n  sample_size = 20480\n  ignore_errors = false\n  all_varchar = 0\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConversionException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# ---- DuckDB load ----\u001b[39;00m\n\u001b[32m     52\u001b[39m con = ddb.connect(DB_PATH)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     54\u001b[39m \u001b[33;43m  CREATE OR REPLACE TABLE events AS\u001b[39;49m\n\u001b[32m     55\u001b[39m \u001b[33;43m  SELECT * FROM read_csv_auto(?, union_by_name=true, header=true, filename=true);\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcsvs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# ---- Inspect available columns ----\u001b[39;00m\n\u001b[32m     59\u001b[39m cols = con.execute(\u001b[33m\"\u001b[39m\u001b[33mPRAGMA table_info(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mevents\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m).df()[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m].str.lower().tolist()\n",
      "\u001b[31mConversionException\u001b[39m: Conversion Error: CSV Error on Line: 1990194\nOriginal Line: \nNA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\nError when converting column \"service_date\". Could not convert string \"NA\" to 'DATE'\n\nColumn service_date is being converted as type DATE\nThis type was auto-detected from the CSV file.\nPossible solutions:\n* Override the type for this column manually by setting the type explicitly, e.g., types={'service_date': 'VARCHAR'}\n* Set the sample size to a larger value to enable the auto-detection to scan more values, e.g., sample_size=-1\n* Use a COPY statement to automatically derive types from an existing table.\n* Check whether the null string value is set correctly (e.g., nullstr = 'N/A')\n\n  file = /tmp/mbta_2022/MBTA_Bus_Arrival_Departure_Times_2022/MBTA-Bus-Arrival-Departure-Times_2022-08.csv\n  delimiter = , (Auto-Detected)\n  quote = \" (Auto-Detected)\n  escape = \" (Auto-Detected)\n  new_line = \\r\\n (Auto-Detected)\n  header = true (Set By User)\n  skip_rows = 0 (Auto-Detected)\n  comment = (empty) (Auto-Detected)\n  strict_mode = true (Auto-Detected)\n  date_format =  (Auto-Detected)\n  timestamp_format =  (Auto-Detected)\n  null_padding = 0\n  sample_size = 20480\n  ignore_errors = false\n  all_varchar = 0\n\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MBTA Bus Reliability — 2024 (DuckDB, adaptive schema)\n",
    "# =========================\n",
    "!pip install -q duckdb gdown pyzipper\n",
    "\n",
    "import os, glob, zipfile, shutil\n",
    "import duckdb as ddb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Config ----\n",
    "YEAR = \"2024\"\n",
    "FILE_ID = \"18b9WsRqftUSlcg7r2KrJcZa7YV6x9g1D\"  # Google Drive file id for 2024 ZIP\n",
    "EXCLUDED_ROUTES = ('8007','7001','RAD','191','192','193','194','743','746_')\n",
    "TARGET_ROUTES   = (\"22\",\"29\",\"15\",\"45\",\"28\",\"44\",\"42\",\"17\",\"23\",\"31\",\"26\",\"111\",\"24\",\"33\",\"14\")\n",
    "ON_TIME_THRESHOLD_MIN = 0.5\n",
    "\n",
    "TMP_DIR = f\"/tmp/mbta_{YEAR}\"\n",
    "ZIP_PATH = f\"/tmp/mbta_{YEAR}.zip\"\n",
    "DB_PATH  = f\"/tmp/mbta_{YEAR}.duckdb\"\n",
    "\n",
    "# Clean temp & create dirs\n",
    "if os.path.exists(TMP_DIR):\n",
    "    shutil.rmtree(TMP_DIR)\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Download & extract ZIP ----\n",
    "import gdown\n",
    "url = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
    "print(f\"Downloading {YEAR} ZIP…\")\n",
    "gdown.download(url, ZIP_PATH, quiet=False)\n",
    "\n",
    "def extract_zip_with_fallback(zip_path, out_dir):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            zf.extractall(out_dir)\n",
    "            return\n",
    "    except (zipfile.BadZipFile, NotImplementedError):\n",
    "        import pyzipper\n",
    "        with pyzipper.AESZipFile(zip_path) as zf:\n",
    "            zf.extractall(out_dir)\n",
    "\n",
    "print(f\"Extracting to {TMP_DIR} …\")\n",
    "extract_zip_with_fallback(ZIP_PATH, TMP_DIR)\n",
    "\n",
    "csvs = glob.glob(os.path.join(TMP_DIR, \"**\", \"*.csv\"), recursive=True)\n",
    "if not csvs:\n",
    "    raise RuntimeError(f\"No CSV files found in {TMP_DIR}\")\n",
    "print(f\"Found {len(csvs)} CSV files for {YEAR}.\")\n",
    "\n",
    "# ---- DuckDB load ----\n",
    "con = ddb.connect(DB_PATH)\n",
    "con.execute(\"\"\"\n",
    "  CREATE OR REPLACE TABLE events AS\n",
    "  SELECT * FROM read_csv_auto(?, union_by_name=true, header=true, filename=true);\n",
    "\"\"\", [csvs])\n",
    "\n",
    "# ---- Inspect available columns ----\n",
    "cols = con.execute(\"PRAGMA table_info('events')\").df()[\"name\"].str.lower().tolist()\n",
    "def pick(*candidates):\n",
    "    for c in candidates:\n",
    "        if c and c.lower() in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# Flexible column mapping\n",
    "route_col      = pick(\"route_id\",\"route\")\n",
    "stop_col       = pick(\"stop_id\",\"time_point_id\",\"stop\")\n",
    "trip_col       = pick(\"trip_id\",\"half_trip_id\")\n",
    "direction_col  = pick(\"direction_id\",\"direction\")\n",
    "service_date_c = pick(\"service_date\",\"date\")\n",
    "scheduled_col  = pick(\"scheduled\",\"scheduled_time\",\"sched_time\")\n",
    "actual_col     = pick(\"actual\",\"actual_time\",\"time\",\"timestamp\",\"departure_time\",\"arrival_time\")\n",
    "delay_col_raw  = pick(\"delay_min\",\"lateness\",\"delay\",\"earliness\")  # 'earliness' will be negated\n",
    "\n",
    "# Build SQL expressions safely\n",
    "route_expr     = f\"CAST({route_col} AS VARCHAR)\" if route_col else \"NULL\"\n",
    "stop_expr      = f\"CAST({stop_col} AS VARCHAR)\" if stop_col else \"NULL\"\n",
    "trip_expr      = f\"CAST({trip_col} AS VARCHAR)\" if trip_col else \"NULL\"\n",
    "direction_expr = f\"CAST({direction_col} AS VARCHAR)\" if direction_col else \"NULL\"\n",
    "service_expr   = f\"CAST({service_date_c} AS VARCHAR)\" if service_date_c else \"NULL\"\n",
    "sched_expr     = f\"CAST({scheduled_col} AS VARCHAR)\" if scheduled_col else \"NULL\"\n",
    "actual_expr    = f\"CAST({actual_col} AS VARCHAR)\" if actual_col else \"NULL\"\n",
    "\n",
    "# Delay expression:\n",
    "#  - if 'delay_min' or 'lateness'/'delay' exists, use it;\n",
    "#  - if only 'earliness' exists, delay_min = -earliness (early => negative delay);\n",
    "\n",
    "def to_double(colname):\n",
    "    # Strip thousands separators; try-cast to DOUBLE\n",
    "    return f\"TRY_CAST(REPLACE({colname}, ',', '') AS DOUBLE)\"\n",
    "    \n",
    "delay_expr = \"NULL\"\n",
    "if delay_col_raw:\n",
    "    if delay_col_raw.lower() == \"earliness\":\n",
    "        delay_expr = f\"(-1.0 * {to_double(delay_col_raw)})\"\n",
    "    else:\n",
    "        delay_expr = to_double(delay_col_raw)\n",
    "\n",
    "# Some datasets already provide headway; detect it\n",
    "headway_col = pick(\"headway\",\"headway_min\",\"avg_headway\",\"wait_time\")\n",
    "\n",
    "# ---------- Build 'clean' view ----------\n",
    "# Parse timestamps only if we have the needed columns; otherwise leave NULL and rely on provided headway\n",
    "# --- Safe timestamp parsing that trims extra timestamps and offsets ---\n",
    "# Extract only the first datetime part (up to first timezone or extra timestamp)\n",
    "# --- Always define time_parse safely as a static SQL block ---\n",
    "# --- Robust timestamp parsing: supports datetime, time-less dates, MM/DD/YYYY too ---\n",
    "# Robust timestamp parsing:\n",
    "# - pull just the first date/time via REGEXP_EXTRACT\n",
    "# - NULLIF('', '') -> NULL\n",
    "# - TRY_STRPTIME(...) returns NULL instead of raising on bad/empty strings\n",
    "time_parse = \"\"\"\n",
    "    /* scheduled_dt */\n",
    "    COALESCE(\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}[ T]\\\\d{2}:\\\\d{2}:\\\\d{2}'), ''), '%Y-%m-%d %H:%M:%S'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}[ T]\\\\d{2}:\\\\d{2}'),    ''), '%Y-%m-%d %H:%M'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}'),                     ''), '%Y-%m-%d'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '[0-9/]{10} \\\\d{2}:\\\\d{2}:\\\\d{2}'),          ''), '%m/%d/%Y %H:%M:%S'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '[0-9/]{10} \\\\d{2}:\\\\d{2}'),                 ''), '%m/%d/%Y %H:%M'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(scheduled_raw, '[0-9/]{10}'),                               ''), '%m/%d/%Y')\n",
    "    ) AS scheduled_dt,\n",
    "\n",
    "    /* actual_dt */\n",
    "    COALESCE(\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}[ T]\\\\d{2}:\\\\d{2}:\\\\d{2}'), ''), '%Y-%m-%d %H:%M:%S'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}[ T]\\\\d{2}:\\\\d{2}'),    ''), '%Y-%m-%d %H:%M'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '\\\\d{4}-\\\\d{2}-\\\\d{2}'),                     ''), '%Y-%m-%d'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '[0-9/]{10} \\\\d{2}:\\\\d{2}:\\\\d{2}'),          ''), '%m/%d/%Y %H:%M:%S'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '[0-9/]{10} \\\\d{2}:\\\\d{2}'),                 ''), '%m/%d/%Y %H:%M'),\n",
    "      TRY_STRPTIME(NULLIF(REGEXP_EXTRACT(actual_raw, '[0-9/]{10}'),                               ''), '%m/%d/%Y')\n",
    "    ) AS actual_dt\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "clean_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW clean AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    {route_expr}     AS route_id,\n",
    "    {stop_expr}      AS stop_id,\n",
    "    {trip_expr}      AS trip_raw,\n",
    "    {direction_expr} AS direction_id,\n",
    "    {service_expr}   AS service_date,\n",
    "    {sched_expr}     AS scheduled_raw,\n",
    "    {actual_expr}    AS actual_raw,\n",
    "    {delay_expr}     AS delay_from_file,\n",
    "    {\"TRY_CAST(\" + headway_col + \" AS DOUBLE)\" if headway_col else \"NULL\"} AS headway_from_file\n",
    "  FROM events\n",
    "),\n",
    "ts AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    {time_parse}\n",
    "  FROM base\n",
    "),\n",
    "enriched AS (\n",
    "  SELECT\n",
    "    route_id, stop_id, trip_raw, direction_id, service_date,\n",
    "    scheduled_dt, actual_dt, headway_from_file,\n",
    "    /* Prefer provided delay; else compute from timestamps if possible */\n",
    "    COALESCE(\n",
    "      delay_from_file,\n",
    "      CASE WHEN scheduled_dt IS NOT NULL AND actual_dt IS NOT NULL\n",
    "           THEN DATE_DIFF('minute', scheduled_dt, actual_dt)::DOUBLE\n",
    "           ELSE NULL END\n",
    "    ) AS delay_min\n",
    "  FROM ts\n",
    "),\n",
    "sequenced AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    CASE\n",
    "      WHEN trip_raw IS NOT NULL THEN trip_raw\n",
    "      ELSE (\n",
    "        COALESCE(route_id,'') || '-' || COALESCE(service_date,'') || '-' || COALESCE(direction_id,'') || '-t' ||\n",
    "        CAST(\n",
    "          ROW_NUMBER() OVER (\n",
    "            PARTITION BY route_id, service_date, direction_id\n",
    "            ORDER BY COALESCE(actual_dt, scheduled_dt)\n",
    "          ) AS BIGINT\n",
    "        )\n",
    "      )\n",
    "    END AS trip_key\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT *\n",
    "FROM sequenced\n",
    "WHERE route_id NOT IN ({\",\".join(\"'\" + r + \"'\" for r in EXCLUDED_ROUTES)})\n",
    "\"\"\"\n",
    "con.execute(clean_sql)\n",
    "\n",
    "# ---------- Q1: End-to-end travel time by route ----------\n",
    "# If we don't have timestamps, this will return empty; that's OK (some sources only have headway/delay).\n",
    "e2e_df = con.execute(\"\"\"\n",
    "  WITH trip_bounds AS (\n",
    "    SELECT route_id, trip_key,\n",
    "           MIN(actual_dt) AS t_min, MAX(actual_dt) AS t_max\n",
    "    FROM clean\n",
    "    WHERE actual_dt IS NOT NULL AND trip_key IS NOT NULL\n",
    "    GROUP BY route_id, trip_key\n",
    "  )\n",
    "  SELECT route_id,\n",
    "         AVG(DATE_DIFF('minute', t_min, t_max))::DOUBLE AS avg_e2e_min,\n",
    "         COUNT(*) AS n_trips\n",
    "  FROM trip_bounds\n",
    "  GROUP BY route_id\n",
    "  ORDER BY avg_e2e_min DESC\n",
    "\"\"\").df()\n",
    "\n",
    "if not e2e_df.empty:\n",
    "    plt.figure()\n",
    "    plt.bar(e2e_df[\"route_id\"].head(25), e2e_df[\"avg_e2e_min\"].head(25))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Minutes\")\n",
    "    plt.title(f\"Average End-to-End Travel Time by Route (Top 25) — {YEAR}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ℹ️ Skipping E2E plot — no usable timestamps to compute trip durations.\")\n",
    "\n",
    "# ---------- Q2: Average wait (headway) — on-time vs delayed ----------\n",
    "if headway_col:\n",
    "    # Use provided headway, then split by status using delay_min\n",
    "    headway_df = con.execute(f\"\"\"\n",
    "      WITH hw AS (\n",
    "        SELECT\n",
    "          CASE WHEN ABS(delay_min) <= {ON_TIME_THRESHOLD_MIN} THEN 'on_time' ELSE 'delayed' END AS status,\n",
    "          headway_from_file AS headway_min\n",
    "        FROM clean\n",
    "        WHERE headway_from_file IS NOT NULL\n",
    "      )\n",
    "      SELECT status, AVG(headway_min) AS avg_headway_min\n",
    "      FROM hw\n",
    "      GROUP BY status\n",
    "      ORDER BY status\n",
    "    \"\"\").df()\n",
    "else:\n",
    "    # Compute headway from timestamps (if available)\n",
    "    headway_df = con.execute(f\"\"\"\n",
    "      WITH ordered AS (\n",
    "        SELECT\n",
    "          route_id, stop_id, actual_dt, delay_min,\n",
    "          LAG(actual_dt) OVER (PARTITION BY route_id, stop_id ORDER BY actual_dt) AS prev_dt\n",
    "        FROM clean\n",
    "        WHERE actual_dt IS NOT NULL\n",
    "      )\n",
    "      SELECT\n",
    "        CASE WHEN ABS(delay_min) <= {ON_TIME_THRESHOLD_MIN} THEN 'on_time' ELSE 'delayed' END AS status,\n",
    "        AVG(DATE_DIFF('minute', prev_dt, actual_dt))::DOUBLE AS avg_headway_min\n",
    "      FROM ordered\n",
    "      WHERE prev_dt IS NOT NULL\n",
    "      GROUP BY status\n",
    "      ORDER BY status\n",
    "    \"\"\").df()\n",
    "\n",
    "if not headway_df.empty:\n",
    "    plt.figure()\n",
    "    plt.bar(headway_df[\"status\"], headway_df[\"avg_headway_min\"])\n",
    "    plt.ylabel(\"Average Headway (min)\")\n",
    "    plt.title(f\"Citywide Average Wait (Headway) — On-Time vs Delayed — {YEAR}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ℹ️ Skipping headway plot — neither provided headway nor timestamps available.\")\n",
    "\n",
    "# ---------- Q3: Citywide average delay ----------\n",
    "city_delay_df = con.execute(\"\"\"\n",
    "  SELECT AVG(delay_min)::DOUBLE AS citywide_avg_delay_min\n",
    "  FROM clean\n",
    "  WHERE delay_min IS NOT NULL\n",
    "\"\"\").df()\n",
    "print(f\"Citywide average delay ({YEAR}): {city_delay_df['citywide_avg_delay_min'][0] if not city_delay_df.empty else 'NA'} min\")\n",
    "\n",
    "# ---------- Q4: Average delay for target routes ----------\n",
    "targets_clause = \"(\" + \",\".join(f\"'{r}'\" for r in TARGET_ROUTES) + \")\"\n",
    "targets_df = con.execute(f\"\"\"\n",
    "  SELECT route_id, AVG(delay_min)::DOUBLE AS avg_delay_min\n",
    "  FROM clean\n",
    "  WHERE delay_min IS NOT NULL\n",
    "    AND route_id IN {targets_clause}\n",
    "  GROUP BY route_id\n",
    "  ORDER BY avg_delay_min DESC\n",
    "\"\"\").df()\n",
    "\n",
    "if not targets_df.empty:\n",
    "    plt.figure()\n",
    "    plt.bar(targets_df[\"route_id\"], targets_df[\"avg_delay_min\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Average Delay (min)\")\n",
    "    plt.title(f\"Average Delay — Target Routes — {YEAR}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ℹ️ No target-route delay results — check that those routes exist in 2018 after exclusions.\")\n",
    "\n",
    "# ---------- Save outputs ----------\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "e2e_df.to_csv(f\"results/e2e_{YEAR}.csv\", index=False)\n",
    "headway_df.to_csv(f\"results/headway_on_time_vs_delayed_{YEAR}.csv\", index=False)\n",
    "city_delay_df.to_csv(f\"results/citywide_avg_delay_{YEAR}.csv\", index=False)\n",
    "targets_df.to_csv(f\"results/avg_delay_targets_{YEAR}.csv\", index=False)\n",
    "print(\"✅ Done. CSVs saved in ./results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5cbe7-7571-481d-a106-cd04b4d4385a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
